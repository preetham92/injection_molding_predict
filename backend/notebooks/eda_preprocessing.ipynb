{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f9f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_preprocessing.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9356c485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully. Shape: (1000, 19)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the dataset\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # Use 'low_memory=False' for potentially large or complex files, ensuring correct dtype inference\n",
    "    df = pd.read_csv('../data/manufacturing_dataset_1000_samples.csv', low_memory=False)\n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file not found. Please check the file path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f49987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped columns: ['Timestamp']\n",
      "\n",
      "Numeric Features (13): ['Injection_Temperature', 'Injection_Pressure', 'Cycle_Time', 'Cooling_Time', 'Material_Viscosity', 'Ambient_Temperature', 'Machine_Age', 'Operator_Experience', 'Maintenance_Hours', 'Temperature_Pressure_Ratio', 'Total_Cycle_Time', 'Efficiency_Score', 'Machine_Utilization']\n",
      "Categorical Features (4): ['Shift', 'Machine_Type', 'Material_Grade', 'Day_of_Week']\n",
      "\n",
      "Fitting preprocessor (handles imputation, scaling, and OHE)...\n",
      "Preprocessor fit successfully.\n",
      "Preprocessor saved as 'preprocessor.pkl'.\n",
      "\n",
      "Transforming features...\n",
      "Preprocessing complete.\n",
      "\n",
      "Shape of processed feature data: (1000, 29)\n",
      "\n",
      "EDA and preprocessing complete. Data is ready for model training.\n"
     ]
    }
   ],
   "source": [
    "# 2. Preprocessing Steps\n",
    "\n",
    "# Define the target and columns to drop\n",
    "target_col = 'Parts_Per_Hour'\n",
    "columns_to_drop = ['Timestamp']\n",
    "\n",
    "# CRITICAL FIX 1: Drop unnecessary columns from the DataFrame immediately\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "if columns_to_drop:\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    print(f\"\\nDropped columns: {columns_to_drop}\")\n",
    "\n",
    "# Define features and target using the CLEANED DataFrame\n",
    "features = df.drop(columns=target_col, errors='ignore')\n",
    "target = df[target_col]\n",
    "\n",
    "# Identify columns based on their data types from the CLEANED features\n",
    "# Note: Dtypes are inferred automatically, ensuring all numeric/categorical features are captured\n",
    "numeric_features = features.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = features.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric Features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical Features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Preprocessing Pipelines\n",
    "# Numerical pipeline: impute missing values and then scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: encode categorical features\n",
    "# CRITICAL FIX 2: Use sparse_output=True to prevent kernel death due to memory\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "# Create a preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    # CRITICAL FIX 3: Use remainder='drop' to ensure only defined columns are processed\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Fit and Save the Preprocessor\n",
    "print(\"\\nFitting preprocessor (handles imputation, scaling, and OHE)...\")\n",
    "preprocessor.fit(features)\n",
    "print(\"Preprocessor fit successfully.\")\n",
    "\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "print(\"Preprocessor saved as 'preprocessor.pkl'.\")\n",
    "\n",
    "# Apply the preprocessor to transform the features\n",
    "print(\"\\nTransforming features...\")\n",
    "# X_processed is now a sparse matrix (or a NumPy array if the data is all numeric)\n",
    "X_processed = preprocessor.transform(features)\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "print(f\"\\nShape of processed feature data: {X_processed.shape}\")\n",
    "# Note: We avoid converting X_processed to a dense DataFrame here to prevent crashes.\n",
    "\n",
    "print(\"\\nEDA and preprocessing complete. Data is ready for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2378edce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determined Project Root: c:\\Users\\LENOVO\\OneDrive\\Desktop\\injection_molding_predictor\\backend\n",
      "Attempting to move file from: c:\\Users\\LENOVO\\OneDrive\\Desktop\\injection_molding_predictor\\backend\\notebooks\\preprocessor.pkl\n",
      "Attempting to move file to: c:\\Users\\LENOVO\\OneDrive\\Desktop\\injection_molding_predictor\\backend\\models\\preprocessor.pkl\n",
      "\n",
      "Successfully moved preprocessor.pkl to backend\\models\\preprocessor.pkl.\n"
     ]
    }
   ],
   "source": [
    "# Final cell in eda_preprocessing.ipynb - ROBUST FILE MOVEMENT\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- 1. Find the true project root directory ---\n",
    "# This logic searches upward from the notebook's location until it finds a marker file (like requirements.txt)\n",
    "# to reliably establish the project root, ignoring redundant paths like 'backend/'.\n",
    "current_dir = os.getcwd()\n",
    "PROJECT_ROOT = current_dir\n",
    "while not os.path.exists(os.path.join(PROJECT_ROOT, 'requirements.txt')) and os.path.dirname(PROJECT_ROOT) != PROJECT_ROOT:\n",
    "    PROJECT_ROOT = os.path.dirname(PROJECT_ROOT)\n",
    "\n",
    "# If the root still seems wrong, default back one step (standard for notebooks in a subfolder)\n",
    "if os.path.dirname(current_dir) == current_dir or 'notebooks' in current_dir:\n",
    "    PROJECT_ROOT = os.path.dirname(current_dir) \n",
    "\n",
    "NOTEBOOKS_DIR = current_dir\n",
    "\n",
    "# --- 2. Define Paths based on the reliable root ---\n",
    "# The source is where the notebook saved the file (its own directory)\n",
    "SOURCE_PATH = os.path.join(NOTEBOOKS_DIR, 'preprocessor.pkl') \n",
    "\n",
    "# The destination is calculated from the true PROJECT_ROOT\n",
    "DESTINATION_PATH = os.path.join(PROJECT_ROOT, 'models', 'preprocessor.pkl')\n",
    "\n",
    "print(f\"\\nDetermined Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Attempting to move file from: {SOURCE_PATH}\")\n",
    "print(f\"Attempting to move file to: {DESTINATION_PATH}\")\n",
    "\n",
    "# --- 3. Execute Move ---\n",
    "try:\n",
    "    # Ensure the destination folder exists before moving\n",
    "    os.makedirs(os.path.dirname(DESTINATION_PATH), exist_ok=True)\n",
    "    \n",
    "    # Move the file\n",
    "    shutil.move(SOURCE_PATH, DESTINATION_PATH)\n",
    "    \n",
    "    print(f\"\\nSuccessfully moved preprocessor.pkl to {os.path.join('backend', 'models', 'preprocessor.pkl')}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: preprocessor.pkl not found at {SOURCE_PATH}.\")\n",
    "    print(\"Please ensure the file was created in the same directory as the notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during move: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0cc0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
